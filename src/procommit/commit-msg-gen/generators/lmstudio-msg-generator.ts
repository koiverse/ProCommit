
import fetch from "node-fetch";
import { MsgGenerator, createDiffAwareUserPrompt, postProcessCommitMessage } from "./msg-generator";
import { getConfiguration } from "@utils/configuration";
import {
  englishInstructions,
  russianInstructions,
  japanInstructions,
  koreanInstructions,
  germanInstructions
} from "@utils/langInstruction";

interface LMStudioConfig {
  apiKey?: string; // not used, but for unified interface
  endpoint?: string;
  model?: string;
  temperature?: number;
  maxTokens?: number;
}

export class LMStudioMsgGenerator implements MsgGenerator {
  endpoint: string;
  model: string;
  temperature?: number;
  maxTokens?: number;

  constructor(config: LMStudioConfig) {
    this.endpoint = config.endpoint || "http://localhost:1234/v1";
    this.model = config.model || "default";
    this.temperature = config.temperature;
    this.maxTokens = config.maxTokens;
  }

  async generate(diff: string): Promise<string> {
    const url = `${this.endpoint.replace(/\/$/, "")}/chat/completions`;
    const config = getConfiguration();
    const language = config.general?.language || "English";
    const includeFileExtension = config.general?.includeFileExtension ?? true;
    let instruction: string;
    switch (language) {
      case "Russian":
        instruction = russianInstructions;
        break;
      case "Japanese":
        instruction = japanInstructions;
        break;
      case "Korean":
        instruction = koreanInstructions;
        break;
      case "German":
        instruction = germanInstructions;
        break;
      case "English":
      default:
        instruction = englishInstructions;
        break;
    }
    const { userPrompt, analysis } = createDiffAwareUserPrompt(diff);
    const body: any = {
      model: this.model,
      messages: [
        { role: "system", content: instruction },
        { role: "user", content: userPrompt }
      ],
      stream: false
    };
    if (this.temperature !== undefined) body.temperature = this.temperature;
    if (this.maxTokens !== undefined) body.max_tokens = this.maxTokens;

    const response = await fetch(url, {
      method: "POST",
      headers: { "Content-Type": "application/json", "Authorization": "Bearer lm-studio" },
      body: JSON.stringify(body)
    });
    if (!response.ok) throw new Error(`LMStudio API error: ${response.statusText}`);
    const data: any = await response.json();
    if (!data.choices || !data.choices[0]?.message?.content) {
      throw new Error("No commit message generated by LMStudio.");
    }
    return postProcessCommitMessage(data.choices[0].message.content, { includeFileExtension, analysis });
  }
}
